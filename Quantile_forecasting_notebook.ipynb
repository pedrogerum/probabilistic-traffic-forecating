{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f7a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  \"Using `json`-module for json-handling. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from mxnet import nd, gpu, gluon, autograd\n",
    "import mxnet as mx\n",
    "import pandas as pd\n",
    "from mxnet import gluon\n",
    "import gc \n",
    "\n",
    "from gluonts.env import env\n",
    "\n",
    "# env._push(use_tqdm=False)\n",
    "\n",
    "import json\n",
    "from gluonts.dataset.common import load_datasets\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.dataset.rolling_dataset import NumSplitsStrategy, StepStrategy, generate_rolling_dataset, truncate_features\n",
    "from gluonts.model.seq2seq import MQRNNEstimator\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.distribution import PiecewiseLinearOutput\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.trainer.model_averaging import SelectNBestMean\n",
    "from gluonts.mx.trainer.model_iteration_averaging import Alpha_Suffix\n",
    "from gluonts.evaluation import Evaluator\n",
    "from pathlib import Path\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from gluonts.env import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed0d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate whatever environment. \n",
    "# cd into the gluonts-local directory\n",
    "# pip install . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909142b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantile_forecast import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93d8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainer(config):\n",
    "    trainer = Trainer(epochs=config[\"epochs\"],\n",
    "                      learning_rate=config[\"learning_rate\"],\n",
    "                      learning_rate_decay_factor=config[\"learning_rate_decay_factor\"], \n",
    "                      weight_decay=config[\"weight_decay\"],\n",
    "                      ctx=mx.cpu())\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707cb286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 474, 'learning_rate': 0.00023946721735196308, 'learning_rate_decay_factor': 0.6898245880780415, 'weight_decay': 5.036032509222127e-09, 'num_quantiles': 23}\n",
      "learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/transform/split.py:36: FutureWarning: Timestamp.freq is deprecated and will be removed in a future version\n",
      "  return _shift_timestamp_helper(ts, ts.freq, offset)\n",
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/transform/feature.py:352: FutureWarning: Timestamp.freq is deprecated and will be removed in a future version\n",
      "  self._min_time_point, self._max_time_point, freq=start.freq\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/pedro/pedro_code/deep-traffic/fresh/data/data_to_tune/chic/chic.gluonts\"\n",
    "model_path = \"./models/chic_mqrnn\"\n",
    "config_path = \"./configs/test/config_chic_5_nonmonot.json\"\n",
    "quantile_boundary = 1e-3\n",
    "# num_pieces = 30\n",
    "monotonic = False\n",
    "prediction_length = 1\n",
    "\n",
    "with open(config_path) as json_file:\n",
    "        config = json.load(json_file)\n",
    "        print(config)\n",
    "config['epochs'] = 5\n",
    "num_quantiles = config[\"num_quantiles\"]\n",
    "dataset = load_dataset(dataset_path)\n",
    "quantiles = make_quantiles(dataset, num_quantiles, quantile_boundary)\n",
    "\n",
    "trainer = make_trainer(config)\n",
    "estimator = make_mqrnn_estimator(trainer, quantiles, monotonic, dataset.metadata.freq, prediction_length)\n",
    "predictor = make_predictor(estimator, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c665f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/dataset/common.py:317: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  timestamp = pd.Timestamp(string, freq=freq)\n",
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/dataset/common.py:320: FutureWarning: Timestamp.freq is deprecated and will be removed in a future version\n",
      "  if isinstance(timestamp.freq, Tick):\n",
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/dataset/common.py:322: FutureWarning: Timestamp.freq is deprecated and will be removed in a future version\n",
      "  timestamp.floor(timestamp.freq), timestamp.freq\n",
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/dataset/common.py:322: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  timestamp.floor(timestamp.freq), timestamp.freq\n",
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/transform/split.py:57: FutureWarning: Timestamp.freq is deprecated and will be removed in a future version\n",
      "  return ts + offset * ts.freq\n",
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/gluonts/model/forecast.py:502: FutureWarning: The 'freq' argument in Timestamp is deprecated and will be removed in a future version.\n",
      "  self.start_date = pd.Timestamp(start_date, freq=freq)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013420820236206055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 128,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868e841ec2c42f3bd1f43cd1f33650a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crps': 0.04201652236507181, '60cover': 0.5234375, '80cover': 0.8828125, '95cover': 0.921875}\n"
     ]
    }
   ],
   "source": [
    "print(\"here\")\n",
    "metrics, forecast, tsit = evaluate(predictor, dataset, quantiles, prediction_length)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec31eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset vadim\n",
      "vadimloaded\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "684311c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE_PATH = \"/mnt/spacious/shared/vadim_all_sensors.csv\"\n",
    "# def get_dataframe(source_path):\n",
    "#     df = pd.read_csv(source_path)\n",
    "#     df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "#     df[\"target\"] = df.groupby(\"DeviceId\").Occupancy.fillna(method=\"ffill\", limit=5)\n",
    "#     df[\"sensor\"], _ = pd.factorize(df[\"DeviceId\"])\n",
    "#     return df\n",
    "# test_breaks = pd.read_csv('/mnt/spacious/shared/vadim_outlier_test.csv', usecols= ['ds',\"sensor\"])\n",
    "\n",
    "\n",
    "SOURCE_PATH =  '/mnt/spacious/shared/data_dir1_may2020.csv'\n",
    "\n",
    "def get_dataframe(source_path):\n",
    "    df = pd.read_csv(source_path)\n",
    "    df[\"ds\"] = None\n",
    "    for sensor in df[\"sensor\"].unique():\n",
    "        df.loc[df[\"sensor\"] == sensor, \"ds\"] = pd.date_range(start='1/1/2008', end='3/1/2009', freq=\"1min\")[:-1]\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df[\"target\"] = df.groupby(\"sensor\").dens_input_nan.fillna(method=\"ffill\", limit=5)\n",
    "    return df\n",
    "\n",
    "test_breaks = pd.read_csv('/mnt/spacious/shared/peter_outlier_test.csv', usecols= ['ds',\"sensor\"])\n",
    "df = get_dataframe(SOURCE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b81ba",
   "metadata": {},
   "source": [
    "### Making Outliers Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c3b30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segments_chicago(df, test_breaks, diffed=False, drift_thresh=360000000.0*4, min_target_length=288//5):\n",
    "    segments = []\n",
    "    item_id = 0\n",
    "    for s in tqdm(df.sensor.unique()):\n",
    "        target_sequence = (df.loc[df.sensor == s, \"target\"]).to_numpy()\n",
    "        temp_df = df[df.sensor == s].reset_index()\n",
    "        outlier_idx = temp_df[temp_df.ds.isin(test_breaks[test_breaks.sensor == s].ds)].index\n",
    "        target_sequence -= np.min(target_sequence)\n",
    "        target_sequence /= np.max(target_sequence)\n",
    "        ds_sequence =  (df.loc[df.sensor == s, \"ds\"]).to_numpy()[1:]\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        diff_seq = np.diff(ds_sequence).astype(int)\n",
    "        m = np.median(diff_seq)\n",
    "#         print(outlier_idx)\n",
    "        \n",
    "        for count, end_idx in enumerate(outlier_idx):\n",
    "            start_idx = 1000\n",
    "            while np.isnan(target_sequence[end_idx-start_idx:end_idx]).sum()>0:\n",
    "                start_idx -= 1\n",
    "                if start_idx == end_idx:\n",
    "                    print(\"here\")\n",
    "                    break\n",
    "\n",
    "            t_seq = target_sequence[end_idx-start_idx:end_idx].astype(np.float32)\n",
    "            \n",
    "            seg = dict(\n",
    "                start=ds_sequence[end_idx-start_idx], \n",
    "                end=ds_sequence[end_idx],\n",
    "                target= t_seq,\n",
    "                feat_static_cat=np.array([s], dtype=int),\n",
    "                item_id = item_id,\n",
    "                real_time = ds_sequence[:end_idx],\n",
    "            )\n",
    "            \n",
    "            if ((end_idx == outlier_idx).sum()) == 1:\n",
    "                segments.append(seg)\n",
    "                \n",
    "        item_id += 1\n",
    "    segments = [s for s in segments if len(s[\"target\"]) > min_target_length]\n",
    "\n",
    "    return segments, item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4413df46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=393722, step=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.sensor==1].reset_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a6e92228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segments_milwaukee(df, test_breaks, diffed=False, drift_thresh=360000000.0*4, min_target_length=288//4):\n",
    "    segments = []\n",
    "\n",
    "    item_id = 0\n",
    "\n",
    "    for s in tqdm(df.sensor.unique()):\n",
    "        target_sequence = (df.loc[df.sensor == s, \"target\"]).to_numpy()[::1] \n",
    "        target_sequence -= np.nanmin(target_sequence)\n",
    "        target_sequence /= np.nanmax(target_sequence)\n",
    "        ds_sequence =  (df.loc[df.sensor == s, \"ds\"]).to_numpy()[::1]\n",
    "        outlier_idx = df[(df.sensor == s) & (df.ds.isin(test_breaks[test_breaks.sensor == s].ds))].index-612000*(s-1)\n",
    "    \n",
    "        for count, end_idx in enumerate(outlier_idx):\n",
    "            start_idx = 1000\n",
    "            while np.isnan(target_sequence[end_idx-start_idx:end_idx]).sum()>0:\n",
    "                start_idx -= 1\n",
    "                if start_idx == end_idx:\n",
    "                    print(\"here\")\n",
    "                    break\n",
    "\n",
    "            t_seq = target_sequence[end_idx-start_idx:end_idx].astype(np.float32)\n",
    "            \n",
    "            seg = dict(\n",
    "                start=ds_sequence[end_idx-start_idx], \n",
    "                end=ds_sequence[end_idx],\n",
    "                target= t_seq,\n",
    "                feat_static_cat=np.array([s], dtype=int),\n",
    "                item_id = item_id,\n",
    "                real_time = ds_sequence[:end_idx],\n",
    "            )\n",
    "            \n",
    "            if ((end_idx == outlier_idx).sum()) == 1:\n",
    "                segments.append(seg)\n",
    "                \n",
    "        item_id += 1   \n",
    "    segments = [s for s in segments if len(s[\"target\"]) > min_target_length]\n",
    "    return segments, item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b157f193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010553121566772461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 21,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1c9ed4f0ef4e3d952e24d5edd6ad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "segments, item_id = make_segments_chicago(df, test_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76e348e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "save_gluon(\"/home/pedro/pedro_code/deep-traffic/fresh/data/chicago_outlier.gluonts\", segments, segments, '5T')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61479ec",
   "metadata": {},
   "source": [
    "### NAIVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "099f5df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda/envs/mxnet_clean/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df['occ'] = (df.Occupancy - df.Occupancy.min())/df.Occupancy.max()\n",
    "df['occ'] = (df.target - df.target.min())/df.target.max()\n",
    "df[\"WeekDay\"] = df.ds.dt.weekday\n",
    "\n",
    "time_list = df.ds.dt.hour.value_counts().index     \n",
    "# sensors = range(21)\n",
    "sensors = range(1,19)\n",
    "quantiles = [0.001, 0.005, 0.025, 0.05, 0.10, 0.25, 0.33, 0.5, 0.66, 0.75, 0.90, 0.95, 0.975, 0.995, 0.999]\n",
    "# df_train = df[df.ds.dt.year < 2012]\n",
    "# df_test = df[df.ds.dt.year >= 2012]\n",
    "df_train = df[df.ds.dt.year <= 2008]\n",
    "df_test = df[df.ds.dt.year >= 2009]\n",
    "forecasts = {}\n",
    "\n",
    "for weekday in range(7):\n",
    "    for time in range(24):\n",
    "        input_to_forecast = (weekday, time)\n",
    "        current_df = df_train[(df.WeekDay == weekday) & (df.ds.dt.hour == time)].occ\n",
    "        forecast = current_df.quantile(quantiles)\n",
    "        forecasts[input_to_forecast] = forecast\n",
    "#         print(input_to_forecast)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3f45caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = df_test.occ.values\n",
    "y_actual_outliers = df_test[df_test.ds.isin(test_breaks.ds)].occ.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "29a862b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_for_forecast = [(weekday, hour) for weekday, hour in zip(df_test.WeekDay.values, df_test.ds.dt.hour.values)]\n",
    "y_pred = np.array([forecasts[input].values for input in inputs_for_forecast])\n",
    "\n",
    "inputs_for_outliers = [(weekday, hour) for weekday, hour in zip(pd.to_datetime(test_breaks.ds).dt.weekday, pd.to_datetime(test_breaks.ds).dt.hour.values)]\n",
    "y_pred_outliers = np.array([forecasts[input].values for input in inputs_for_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fd85ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quantile_forecast import *\n",
    "\n",
    "def crps(tau, F_inv, x, n_steps=10000, miny=-1, maxy=100):\n",
    "    F = interpolate.interp1d(F_inv, tau, kind=\"linear\", bounds_error=False, fill_value=(0, 1))\n",
    "    y_below = np.linspace(miny, x, n_steps)\n",
    "    y_above = np.linspace(x, maxy, n_steps)\n",
    "    \n",
    "    error_below = np.power(F(y_below), 2)*np.ptp(y_below)/y_below.shape[0]\n",
    "    error_above = np.power(1 - F(y_above), 2)*np.ptp(y_above)/y_above.shape[0]\n",
    "    return (np.sum(error_below)  + np.sum(error_above))\n",
    "\n",
    "def average_crps(quantiles, y_pred, y_actual):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    nb_samples = y_pred.shape[0]\n",
    "    for i in tqdm(range(nb_samples)):\n",
    "        c = crps(quantiles, y_pred[i, :], y_actual[i])\n",
    "        if not np.isnan(c):\n",
    "            total += c\n",
    "            count += 1\n",
    "    return total/max(count, 1e-16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8e51110c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010554075241088867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 13903,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5db8b8495e2409ab3e5e8aa4e0e7404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.017645062286443684"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_crps(quantiles, y_pred[::110], y_actual[::110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0db25e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010447978973388672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 60,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 39,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7f111050e8426caedc02ea1fa1941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.045051142659577326"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_crps(quantiles, y_pred_outliers[::110], y_actual_outliers[::110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "de9e403c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28902, 15)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[::100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1ad1db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(quantiles, pred, y_actual):\n",
    "    print(\"target | actual\")\n",
    "    target = []\n",
    "    actual = []\n",
    "    for i in range(len(quantiles)//2):\n",
    "        lower_pred = pred[:, i]\n",
    "        upper_pred = pred[:, -(i + 1)]\n",
    "        target.append(quantiles[-(i + 1)] - quantiles[i])\n",
    "        actual.append(np.mean(np.logical_and(lower_pred <= y_actual, y_actual <= upper_pred)))\n",
    "#         print(f\"{target:5.4f} | {actual:5.4f} | {(target-actual)/target:5.4f}\")\n",
    "\n",
    "    return (target, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "acd62c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target | actual\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.998, 0.99, 0.95, 0.8999999999999999, 0.8, 0.5, 0.33],\n",
       " [0.945000261560996,\n",
       "  0.9422832967147939,\n",
       "  0.923825591127851,\n",
       "  0.891197164678803,\n",
       "  0.8307471489851433,\n",
       "  0.6032780131826742,\n",
       "  0.4790640039757271])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage(quantiles, y_pred, y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e09fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
